{"cells":[{"cell_type":"code","source":["%scala\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming.twitter\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming._\nimport org.apache.spark.{SparkContext, SparkConf}\nimport org.apache.spark.storage.StorageLevel\nimport twitter4j._\nimport twitter4j.auth.Authorization\nimport twitter4j.conf.ConfigurationBuilder\nimport twitter4j.auth.OAuthAuthorization\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.receiver.Receiver\nimport com.google.gson.Gson\nimport org.apache.spark.streaming.twitter.TwitterUtils\nimport twitter4j.Twitter\nimport twitter4j.TwitterFactory\nimport twitter4j.auth.AccessToken\nimport twitter4j.ResponseList\nimport twitter4j.QueryResult\nimport twitter4j.Query\nimport twitter4j.Paging\nimport scala.sys.process._\nimport scala.collection.mutable.ArrayBuffer"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\nimport org.apache.spark.streaming.twitter.TwitterUtils\nimport org.apache.spark.streaming.StreamingContext\nimport java.util.Properties\nimport org.apache.spark.streaming.twitter.TwitterInputDStream\n\n\n\nval consumerKey = \"test\"\nval consumerSecret = \"test\"\nval accessToken = \"test\"\nval accessTokenSecret = \"test\"\n\n\nSystem.setProperty(\"twitter4j.oauth.consumerKey\", consumerKey)\nSystem.setProperty(\"twitter4j.oauth.consumerSecret\", consumerSecret)\nSystem.setProperty(\"twitter4j.oauth.accessToken\", accessToken)\nSystem.setProperty(\"twitter4j.oauth.accessTokenSecret\", accessTokenSecret)\n\n\nprintln(\"\")\nprintln(\"Create stream\")\nprintln(\"=============\")\nprintln(\"\")\nval filter = Array(\"test\")\nval ssc = new StreamingContext(sc, Seconds(30))\nval stream = TwitterUtils.createStream(ssc, None, filter)\nprintln(\"\")\nprintln(\"Extract texts and print\")\nprintln(\"=======================\")\nprintln(\"\")\n\n\nval statuses = stream.map(status => status.getText())\nstatuses.print()\n\nvar tweets = new ArrayBuffer[String]();\nstatuses.foreachRDD {\n    tweets ++= _.collect() //you can now put it in an array or d w/e you want with it\n}\n\nprintln(\" \")\nprintln(\"Start\")\nprintln(\"=====\")\nprintln(\"\")\nssc.start()\n//ssc.awaitTermination()\nssc.awaitTerminationOrTimeout(180000) \n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\nval tweets_text = tweets.mkString(\"|\\n\\n\")\nprintln(tweets_text)\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\ndbutils.fs.rm(\"file:/dbfs/tmp/test\")\n// Write a file using the local file I/O API (over the fuse mount).\ndbutils.fs.put(\"file:/dbfs/tmp/test\", tweets_text)\n\n// Unless you call this, the code below might not see the file or its latest contents.\n\"sync /dbfs/tmp/test\" !\n\n// Read the file using \"dbfs:/\" instead of the fuse mount.\ndbutils.fs.head(\"dbfs:/tmp/test\")"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"Spark Streaming on Twitter","notebookId":3700315452641205},"nbformat":4,"nbformat_minor":0}
